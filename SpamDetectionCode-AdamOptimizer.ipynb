{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code below will run with Python 3.x\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing data file\n",
    "from pandas import read_csv\n",
    "data= read_csv(\"spambase/spambase.data\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9  ...    48     49  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00 ...  0.00  0.000   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94 ...  0.00  0.132   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25 ...  0.01  0.143   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.137   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.135   \n",
       "\n",
       "    50     51     52     53     54   55    56  57  \n",
       "0  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating features and target\n",
    "features=data.iloc[:,:-1]\n",
    "target=data.iloc[:,57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9   ...    47    48  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...   0.0  0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...   0.0  0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...   0.0  0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...   0.0  0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...   0.0  0.00   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target=pd.get_dummies(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  0  1\n",
       "1  0  1\n",
       "2  0  1\n",
       "3  0  1\n",
       "4  0  1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features=np.array(features)\n",
    "target=np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   6.40000000e-01,   6.40000000e-01, ...,\n",
       "          3.75600000e+00,   6.10000000e+01,   2.78000000e+02],\n",
       "       [  2.10000000e-01,   2.80000000e-01,   5.00000000e-01, ...,\n",
       "          5.11400000e+00,   1.01000000e+02,   1.02800000e+03],\n",
       "       [  6.00000000e-02,   0.00000000e+00,   7.10000000e-01, ...,\n",
       "          9.82100000e+00,   4.85000000e+02,   2.25900000e+03],\n",
       "       ..., \n",
       "       [  3.00000000e-01,   0.00000000e+00,   3.00000000e-01, ...,\n",
       "          1.40400000e+00,   6.00000000e+00,   1.18000000e+02],\n",
       "       [  9.60000000e-01,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          1.14700000e+00,   5.00000000e+00,   7.80000000e+01],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   6.50000000e-01, ...,\n",
       "          1.25000000e+00,   5.00000000e+00,   4.00000000e+01]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       ..., \n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4601, 57)\n",
      "(4601, 2)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print (target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dividing train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train,features_test,target_train,target_test=train_test_split(features,target,test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X=StandardScaler()\n",
    "features_train=sc_X.fit_transform(features_train)\n",
    "features_test=sc_X.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn import preprocessing\n",
    "features_train=preprocessing.scale(features_train)\n",
    "features_test=preprocessing.scale(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3450, 57)\n",
      "(1151, 57)\n",
      "(3450, 2)\n",
      "(1151, 2)\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape)\n",
    "print(features_test.shape)\n",
    "print(target_train.shape)\n",
    "print(target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Classifier Parameters1 for AdamOptimizer\n",
    "logs_path='tmp/practice/v3'\n",
    "save_path=logs_path + '/'\n",
    "n_features = 57\n",
    "n_classes = 2\n",
    "batch_size = 100\n",
    "training_epochs = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 512\n",
    "L = 256\n",
    "M = 128\n",
    "N = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build Graph - Define Tensorflow Operations\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    # None -> batch size can be any size, with n_features\n",
    "    x = tf.placeholder(tf.float32, shape=[None, n_features], name=\"x-input\") \n",
    "    # target n_classes output classes\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, n_classes], name=\"y-input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize a layer 1 of 512 neurons\n",
    "with tf.name_scope(\"Layer1\"):\n",
    "    W1 = tf.Variable(tf.truncated_normal([n_features, K] ,stddev=0.1))\n",
    "    b1 = tf.Variable(tf.ones([K]))\n",
    "    Y1 = tf.nn.sigmoid(tf.matmul(x, W1) + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize a layer 2 of 256 neurons\n",
    "with tf.name_scope(\"Layer2\"):\n",
    "    W2 = tf.Variable(tf.truncated_normal([K, L] ,stddev=0.1))\n",
    "    b2 = tf.Variable(tf.ones([L]))\n",
    "    Y2 = tf.nn.sigmoid(tf.matmul(Y1, W2) + b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize a layer 3 of 128 neurons\n",
    "with tf.name_scope(\"Layer3\"):\n",
    "    W3 = tf.Variable(tf.truncated_normal([L, M] ,stddev=0.1))\n",
    "    b3 = tf.Variable(tf.ones([M]))\n",
    "    Y3 = tf.nn.sigmoid(tf.matmul(Y2, W3) + b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize a layer 4 of 30 neurons\n",
    "with tf.name_scope(\"Layer4\"):\n",
    "    W4 = tf.Variable(tf.truncated_normal([M, N] ,stddev=0.1))\n",
    "    b4 = tf.Variable(tf.ones([N]))\n",
    "    Y4 = tf.nn.sigmoid(tf.matmul(Y3, W4) + b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implement model\n",
    "with tf.name_scope(\"Output\"):\n",
    "    # y is our prediction\n",
    "    W = tf.Variable(tf.truncated_normal([N, n_classes] ,stddev=0.1))\n",
    "    b = tf.Variable(tf.zeros([n_classes]))   \n",
    "    #y = tf.nn.softmax(tf.matmul(Y4, W) + b)\n",
    "    Ylogits = tf.matmul(Y4, W) + b\n",
    "    y = tf.nn.softmax(Ylogits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify cost function\n",
    "with tf.name_scope('Loss'):\n",
    "    # this is our cost\n",
    "    #cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=y_)\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify optimizer-- AdamOptimizer\n",
    "with tf.name_scope('train'):\n",
    "    # optimizer is an \"operation\" which we can execute in a session\n",
    "    train_op = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Accuracy'):\n",
    "    # Prediction\n",
    "    prediction = tf.argmax(y,1,name=\"Predict\")\n",
    "    #Accuracy\n",
    "    correct_prediction = tf.equal(prediction, tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a summary for our cost and accuracy\n",
    "training_loss = tf.summary.scalar(\"training_loss\", cross_entropy)\n",
    "training_accuracy = tf.summary.scalar(\"training_accuracy\", accuracy)\n",
    "test_loss = tf.summary.scalar(\"test_loss\", cross_entropy)\n",
    "test_accuracy = tf.summary.scalar(\"test_accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a Saver to save the graph\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss at step:  0  is  67.3515\n",
      "Epoch:  0\n",
      "Test loss at step:  1  is  67.0075\n",
      "Test loss at step:  2  is  66.6722\n",
      "Test loss at step:  3  is  65.9526\n",
      "Test loss at step:  4  is  64.4887\n",
      "Test loss at step:  5  is  61.7072\n",
      "Epoch:  5\n",
      "Test loss at step:  6  is  56.8219\n",
      "Test loss at step:  7  is  49.9653\n",
      "Test loss at step:  8  is  42.8184\n",
      "Test loss at step:  9  is  37.0396\n",
      "Test loss at step:  10  is  33.0115\n",
      "Epoch:  10\n",
      "Test loss at step:  11  is  30.3645\n",
      "Test loss at step:  12  is  28.6435\n",
      "Test loss at step:  13  is  27.506\n",
      "Test loss at step:  14  is  26.7289\n",
      "Test loss at step:  15  is  26.1787\n",
      "Epoch:  15\n",
      "Test loss at step:  16  is  25.776\n",
      "Test loss at step:  17  is  25.4714\n",
      "Test loss at step:  18  is  25.2341\n",
      "Test loss at step:  19  is  25.0444\n",
      "Test loss at step:  20  is  24.8906\n",
      "Epoch:  20\n",
      "Test loss at step:  21  is  24.7652\n",
      "Test loss at step:  22  is  24.6634\n",
      "Test loss at step:  23  is  24.5813\n",
      "Test loss at step:  24  is  24.5158\n",
      "Test loss at step:  25  is  24.4639\n",
      "Epoch:  25\n",
      "Test loss at step:  26  is  24.4231\n",
      "Test loss at step:  27  is  24.3911\n",
      "Test loss at step:  28  is  24.3659\n",
      "Test loss at step:  29  is  24.3461\n",
      "Test loss at step:  30  is  24.3304\n",
      "Epoch:  30\n",
      "Test loss at step:  31  is  24.3177\n",
      "Test loss at step:  32  is  24.307\n",
      "Test loss at step:  33  is  24.2974\n",
      "Test loss at step:  34  is  24.2876\n",
      "Test loss at step:  35  is  24.2763\n",
      "Epoch:  35\n",
      "Test loss at step:  36  is  24.2622\n",
      "Test loss at step:  37  is  24.2437\n",
      "Test loss at step:  38  is  24.2191\n",
      "Test loss at step:  39  is  24.1868\n",
      "Test loss at step:  40  is  24.1453\n",
      "Epoch:  40\n",
      "Test loss at step:  41  is  24.093\n",
      "Test loss at step:  42  is  24.0289\n",
      "Test loss at step:  43  is  23.9524\n",
      "Test loss at step:  44  is  23.8646\n",
      "Test loss at step:  45  is  23.768\n",
      "Epoch:  45\n",
      "Test loss at step:  46  is  23.6678\n",
      "Test loss at step:  47  is  23.5713\n",
      "Test loss at step:  48  is  23.4859\n",
      "Test loss at step:  49  is  23.4172\n",
      "Test loss at step:  50  is  23.367\n",
      "Epoch:  50\n",
      "Test loss at step:  51  is  23.3337\n",
      "Test loss at step:  52  is  23.314\n",
      "Test loss at step:  53  is  23.3044\n",
      "Test loss at step:  54  is  23.3016\n",
      "Test loss at step:  55  is  23.3032\n",
      "Epoch:  55\n",
      "Test loss at step:  56  is  23.3075\n",
      "Test loss at step:  57  is  23.3134\n",
      "Test loss at step:  58  is  23.3202\n",
      "Test loss at step:  59  is  23.3273\n",
      "Test loss at step:  60  is  23.3346\n",
      "Epoch:  60\n",
      "Test loss at step:  61  is  23.342\n",
      "Test loss at step:  62  is  23.3495\n",
      "Test loss at step:  63  is  23.3575\n",
      "Test loss at step:  64  is  23.3659\n",
      "Test loss at step:  65  is  23.375\n",
      "Epoch:  65\n",
      "Test loss at step:  66  is  23.3835\n",
      "Test loss at step:  67  is  23.3879\n",
      "Test loss at step:  68  is  23.3834\n",
      "Test loss at step:  69  is  23.3709\n",
      "Test loss at step:  70  is  23.3575\n",
      "Epoch:  70\n",
      "Test loss at step:  71  is  23.3459\n",
      "Test loss at step:  72  is  23.3349\n",
      "Test loss at step:  73  is  23.3236\n",
      "Test loss at step:  74  is  23.3114\n",
      "Test loss at step:  75  is  23.298\n",
      "Epoch:  75\n",
      "Test loss at step:  76  is  23.2833\n",
      "Test loss at step:  77  is  23.267\n",
      "Test loss at step:  78  is  23.2493\n",
      "Test loss at step:  79  is  23.2299\n",
      "Test loss at step:  80  is  23.209\n",
      "Epoch:  80\n",
      "Test loss at step:  81  is  23.1865\n",
      "Test loss at step:  82  is  23.1625\n",
      "Test loss at step:  83  is  23.1371\n",
      "Test loss at step:  84  is  23.1105\n",
      "Test loss at step:  85  is  23.0828\n",
      "Epoch:  85\n",
      "Test loss at step:  86  is  23.0544\n",
      "Test loss at step:  87  is  23.0255\n",
      "Test loss at step:  88  is  22.9965\n",
      "Test loss at step:  89  is  22.9676\n",
      "Test loss at step:  90  is  22.9392\n",
      "Epoch:  90\n",
      "Test loss at step:  91  is  22.9114\n",
      "Test loss at step:  92  is  22.8844\n",
      "Test loss at step:  93  is  22.8583\n",
      "Test loss at step:  94  is  22.833\n",
      "Test loss at step:  95  is  22.8086\n",
      "Epoch:  95\n",
      "Test loss at step:  96  is  22.785\n",
      "Test loss at step:  97  is  22.7622\n",
      "Test loss at step:  98  is  22.74\n",
      "Test loss at step:  99  is  22.7186\n",
      "Test loss at step:  100  is  22.6977\n",
      "Epoch:  100\n",
      "Test loss at step:  101  is  22.6774\n",
      "Test loss at step:  102  is  22.6576\n",
      "Test loss at step:  103  is  22.6383\n",
      "Test loss at step:  104  is  22.6195\n",
      "Test loss at step:  105  is  22.6011\n",
      "Epoch:  105\n",
      "Test loss at step:  106  is  22.5831\n",
      "Test loss at step:  107  is  22.5656\n",
      "Test loss at step:  108  is  22.5484\n",
      "Test loss at step:  109  is  22.5316\n",
      "Test loss at step:  110  is  22.5152\n",
      "Epoch:  110\n",
      "Test loss at step:  111  is  22.4991\n",
      "Test loss at step:  112  is  22.4834\n",
      "Test loss at step:  113  is  22.468\n",
      "Test loss at step:  114  is  22.4529\n",
      "Test loss at step:  115  is  22.4382\n",
      "Epoch:  115\n",
      "Test loss at step:  116  is  22.4238\n",
      "Test loss at step:  117  is  22.4097\n",
      "Test loss at step:  118  is  22.3959\n",
      "Test loss at step:  119  is  22.3824\n",
      "Test loss at step:  120  is  22.3693\n",
      "Epoch:  120\n",
      "Test loss at step:  121  is  22.3564\n",
      "Test loss at step:  122  is  22.3438\n",
      "Test loss at step:  123  is  22.3315\n",
      "Test loss at step:  124  is  22.3195\n",
      "Test loss at step:  125  is  22.3077\n",
      "Epoch:  125\n",
      "Test loss at step:  126  is  22.2962\n",
      "Test loss at step:  127  is  22.285\n",
      "Test loss at step:  128  is  22.274\n",
      "Test loss at step:  129  is  22.2633\n",
      "Test loss at step:  130  is  22.2528\n",
      "Epoch:  130\n",
      "Test loss at step:  131  is  22.2426\n",
      "Test loss at step:  132  is  22.2326\n",
      "Test loss at step:  133  is  22.2228\n",
      "Test loss at step:  134  is  22.2133\n",
      "Test loss at step:  135  is  22.204\n",
      "Epoch:  135\n",
      "Test loss at step:  136  is  22.1948\n",
      "Test loss at step:  137  is  22.1859\n",
      "Test loss at step:  138  is  22.1772\n",
      "Test loss at step:  139  is  22.1687\n",
      "Test loss at step:  140  is  22.1604\n",
      "Epoch:  140\n",
      "Test loss at step:  141  is  22.1523\n",
      "Test loss at step:  142  is  22.1443\n",
      "Test loss at step:  143  is  22.1366\n",
      "Test loss at step:  144  is  22.129\n",
      "Test loss at step:  145  is  22.1216\n",
      "Epoch:  145\n",
      "Test loss at step:  146  is  22.1143\n",
      "Test loss at step:  147  is  22.1072\n",
      "Test loss at step:  148  is  22.1003\n",
      "Test loss at step:  149  is  22.0935\n",
      "Test loss at step:  150  is  22.0868\n",
      "Epoch:  150\n",
      "Test loss at step:  151  is  22.0803\n",
      "Test loss at step:  152  is  22.0739\n",
      "Test loss at step:  153  is  22.0677\n",
      "Test loss at step:  154  is  22.0616\n",
      "Test loss at step:  155  is  22.0556\n",
      "Epoch:  155\n",
      "Test loss at step:  156  is  22.0497\n",
      "Test loss at step:  157  is  22.0439\n",
      "Test loss at step:  158  is  22.0382\n",
      "Test loss at step:  159  is  22.0326\n",
      "Test loss at step:  160  is  22.0272\n",
      "Epoch:  160\n",
      "Test loss at step:  161  is  22.0218\n",
      "Test loss at step:  162  is  22.0165\n",
      "Test loss at step:  163  is  22.0113\n",
      "Test loss at step:  164  is  22.0062\n",
      "Test loss at step:  165  is  22.0012\n",
      "Epoch:  165\n",
      "Test loss at step:  166  is  21.9962\n",
      "Test loss at step:  167  is  21.9913\n",
      "Test loss at step:  168  is  21.9866\n",
      "Test loss at step:  169  is  21.9818\n",
      "Test loss at step:  170  is  21.9771\n",
      "Epoch:  170\n",
      "Test loss at step:  171  is  21.9725\n",
      "Test loss at step:  172  is  21.968\n",
      "Test loss at step:  173  is  21.9635\n",
      "Test loss at step:  174  is  21.9591\n",
      "Test loss at step:  175  is  21.9547\n",
      "Epoch:  175\n",
      "Test loss at step:  176  is  21.9504\n",
      "Test loss at step:  177  is  21.9461\n",
      "Test loss at step:  178  is  21.9418\n",
      "Test loss at step:  179  is  21.9376\n",
      "Test loss at step:  180  is  21.9334\n",
      "Epoch:  180\n",
      "Test loss at step:  181  is  21.9292\n",
      "Test loss at step:  182  is  21.9251\n",
      "Test loss at step:  183  is  21.921\n",
      "Test loss at step:  184  is  21.9169\n",
      "Test loss at step:  185  is  21.9129\n",
      "Epoch:  185\n",
      "Test loss at step:  186  is  21.9088\n",
      "Test loss at step:  187  is  21.9048\n",
      "Test loss at step:  188  is  21.9008\n",
      "Test loss at step:  189  is  21.8967\n",
      "Test loss at step:  190  is  21.8927\n",
      "Epoch:  190\n",
      "Test loss at step:  191  is  21.8887\n",
      "Test loss at step:  192  is  21.8847\n",
      "Test loss at step:  193  is  21.8807\n",
      "Test loss at step:  194  is  21.8766\n",
      "Test loss at step:  195  is  21.8726\n",
      "Epoch:  195\n",
      "Test loss at step:  196  is  21.8685\n",
      "Test loss at step:  197  is  21.8644\n",
      "Test loss at step:  198  is  21.8603\n",
      "Test loss at step:  199  is  21.8562\n",
      "Test loss at step:  200  is  21.8521\n",
      "Epoch:  200\n",
      "Test loss at step:  201  is  21.8479\n",
      "Test loss at step:  202  is  21.8436\n",
      "Test loss at step:  203  is  21.8394\n",
      "Test loss at step:  204  is  21.8351\n",
      "Test loss at step:  205  is  21.8307\n",
      "Epoch:  205\n",
      "Test loss at step:  206  is  21.8263\n",
      "Test loss at step:  207  is  21.8218\n",
      "Test loss at step:  208  is  21.8172\n",
      "Test loss at step:  209  is  21.8126\n",
      "Test loss at step:  210  is  21.8078\n",
      "Epoch:  210\n",
      "Test loss at step:  211  is  21.803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss at step:  212  is  21.7982\n",
      "Test loss at step:  213  is  21.7932\n",
      "Test loss at step:  214  is  21.7881\n",
      "Test loss at step:  215  is  21.783\n",
      "Epoch:  215\n",
      "Test loss at step:  216  is  21.7778\n",
      "Test loss at step:  217  is  21.7726\n",
      "Test loss at step:  218  is  21.7674\n",
      "Test loss at step:  219  is  21.7622\n",
      "Test loss at step:  220  is  21.7573\n",
      "Epoch:  220\n",
      "Test loss at step:  221  is  21.7527\n",
      "Test loss at step:  222  is  21.7486\n",
      "Test loss at step:  223  is  21.7456\n",
      "Test loss at step:  224  is  21.744\n",
      "Test loss at step:  225  is  21.7446\n",
      "Epoch:  225\n",
      "Test loss at step:  226  is  21.7481\n",
      "Test loss at step:  227  is  21.7551\n",
      "Test loss at step:  228  is  21.7655\n",
      "Test loss at step:  229  is  21.7784\n",
      "Test loss at step:  230  is  21.7926\n",
      "Epoch:  230\n",
      "Test loss at step:  231  is  21.8065\n",
      "Test loss at step:  232  is  21.8193\n",
      "Test loss at step:  233  is  21.8305\n",
      "Test loss at step:  234  is  21.84\n",
      "Test loss at step:  235  is  21.848\n",
      "Epoch:  235\n",
      "Test loss at step:  236  is  21.8547\n",
      "Test loss at step:  237  is  21.8603\n",
      "Test loss at step:  238  is  21.8651\n",
      "Test loss at step:  239  is  21.8691\n",
      "Test loss at step:  240  is  21.8726\n",
      "Epoch:  240\n",
      "Test loss at step:  241  is  21.8755\n",
      "Test loss at step:  242  is  21.8781\n",
      "Test loss at step:  243  is  21.8803\n",
      "Test loss at step:  244  is  21.8823\n",
      "Test loss at step:  245  is  21.8839\n",
      "Epoch:  245\n",
      "Test loss at step:  246  is  21.8854\n",
      "Test loss at step:  247  is  21.8866\n",
      "Test loss at step:  248  is  21.8876\n",
      "Test loss at step:  249  is  21.8884\n",
      "Test loss at step:  250  is  21.889\n",
      "Epoch:  250\n",
      "Test loss at step:  251  is  21.8894\n",
      "Test loss at step:  252  is  21.8897\n",
      "Test loss at step:  253  is  21.8898\n",
      "Test loss at step:  254  is  21.8898\n",
      "Test loss at step:  255  is  21.8895\n",
      "Epoch:  255\n",
      "Accuracy:  0.932233\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Start Graph execution\n",
    "with tf.Session() as sess:\n",
    "    # variables need to be initialized before we can use them\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # create log writer object\n",
    "    writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "    # perform training cycles\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # number of batches in one epoch\n",
    "        batch_count = int(features_train.shape[0]/batch_size)\n",
    "        \n",
    "        for i in range(batch_count):\n",
    "            batch_x  = features_train[i*batch_size:i*batch_size+batch_size]\n",
    "            batch_y  = target_train[i*batch_size:i*batch_size+batch_size]\n",
    "\n",
    "            # perform the operations we defined earlier on batch\n",
    "            _,acc,loss = sess.run([train_op, training_accuracy,training_loss], feed_dict={x: batch_x, y_: batch_y})\n",
    "            \n",
    "            #log training accuracy and loss\n",
    "            writer.add_summary(acc, epoch * batch_count + i)\n",
    "            writer.add_summary(loss, epoch * batch_count + i)    \n",
    "                        \n",
    "        #Test loss and accuracy\n",
    "        acc,loss,a_loss = sess.run([test_accuracy,test_loss,cross_entropy],\n",
    "                                   feed_dict={x: features_test, y_: target_test})\n",
    "        writer.add_summary(acc, epoch * batch_count + i)\n",
    "        writer.add_summary(loss, epoch * batch_count + i)\n",
    "        print ('Test loss at step: ', epoch, ' is ', a_loss) \n",
    "        if epoch % 5 == 0: \n",
    "            print (\"Epoch: \", epoch)\n",
    "                \n",
    "    print (\"Accuracy: \", accuracy.eval(feed_dict={x: features_test, y_: target_test}))\n",
    "    \n",
    "    #Save the model\n",
    "    saver.save(sess, save_path + \"model.ckpt\")\n",
    "    print (\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
